<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/bio.jpg"/><link rel="stylesheet" href="/_next/static/css/78c4370b96c05382.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-480fe316c9db0337.js" async=""></script><script src="/_next/static/chunks/461-a9a5d7b05a71d978.js" async=""></script><script src="/_next/static/chunks/874-6cc630662f3664af.js" async=""></script><script src="/_next/static/chunks/291-e39730cc9327e39b.js" async=""></script><script src="/_next/static/chunks/app/layout-4cf029633e6d5428.js" async=""></script><script src="/_next/static/chunks/178-595a94b9af1e67b5.js" async=""></script><script src="/_next/static/chunks/748-97f90665cfe632a2.js" async=""></script><script src="/_next/static/chunks/app/page-6c80dddde2d5f2d2.js" async=""></script><link rel="icon" href="/favicon.ico" type="image/svg+xml"/><link rel="dns-prefetch" href="https://jialeliu.com"/><link rel="preconnect" href="https://jialeliu.com" crossorigin=""/><link rel="preload" as="font" type="font/woff2" href="https://jialeliu.com/fonts/georgiab.woff2" crossorigin=""/><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><script>
    try {
      const cfg = {"enabled":true,"locales":["en"],"defaultLocale":"en","mode":"auto","fixedLocale":"en","persist":true,"switcher":true,"labels":{"en":"English"}};
      const storageKey = 'locale-storage';
      const normalize = (value) => typeof value === 'string' ? value.trim().replace('_', '-').toLowerCase() : '';
      const matchLocale = (candidate) => {
        const normalized = normalize(candidate);
        if (!normalized) return null;
        if (cfg.locales.includes(normalized)) return normalized;
        const language = normalized.split('-')[0];
        if (cfg.locales.includes(language)) return language;
        return null;
      };

      let resolved = null;

      if (cfg.persist) {
        resolved = matchLocale(localStorage.getItem(storageKey));
      }

      if (!resolved) {
        if (cfg.mode === 'fixed') {
          resolved = cfg.fixedLocale;
        } else {
          resolved = matchLocale(navigator.language);
        }
      }

      if (!resolved) {
        resolved = cfg.defaultLocale;
      }

      const root = document.documentElement;
      root.lang = resolved;
      root.setAttribute('data-locale', resolved);

      if (cfg.persist) {
        localStorage.setItem(storageKey, resolved);
      }
    } catch (e) {
      const root = document.documentElement;
      root.lang = 'en';
      root.setAttribute('data-locale', 'en');
    }
  </script><title>Jingyao Wang</title><meta name="description" content="PhD student at the University of Example."/><meta name="author" content="Jingyao Wang"/><meta name="keywords" content="Jingyao Wang,PhD,Research,Institute of Software Chinese Academy of Sciences"/><meta name="creator" content="Jingyao Wang"/><meta name="publisher" content="Jingyao Wang"/><meta property="og:title" content="Jingyao Wang"/><meta property="og:description" content="PhD student at the University of Example."/><meta property="og:site_name" content="Jingyao Wang&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Jingyao Wang"/><meta name="twitter:description" content="PhD student at the University of Example."/><link rel="icon" href="/favicon.ico"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Jingyao Wang</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-3"><div class="relative flex items-baseline space-x-1"><a data-nav-href="/" class="relative px-3 py-2 text-sm font-medium rounded-lg transition-colors duration-150 text-primary" href="/">About</a><a data-nav-href="/publications" class="relative px-3 py-2 text-sm font-medium rounded-lg transition-colors duration-150 text-neutral-600" href="/publications/">Publications</a><a data-nav-href="/teaching" class="relative px-3 py-2 text-sm font-medium rounded-lg transition-colors duration-150 text-neutral-600" href="/teaching/">Teaching</a><a data-nav-href="/awards" class="relative px-3 py-2 text-sm font-medium rounded-lg transition-colors duration-150 text-neutral-600" href="/awards/">Awards</a><a data-nav-href="/services" class="relative px-3 py-2 text-sm font-medium rounded-lg transition-colors duration-150 text-neutral-600" href="/services/">Services</a><a data-nav-href="/cv" class="relative px-3 py-2 text-sm font-medium rounded-lg transition-colors duration-150 text-neutral-600" href="/cv/">CV</a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-Â«R7pdbÂ»" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Jingyao Wang" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/bio.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Jingyao Wang</h1><p class="text-lg text-accent font-medium mb-1">PhD Student</p><p class="text-neutral-600 mb-2">Institute of Software Chinese Academy of Sciences</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Location"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M15 10.5a3 3 0 1 1-6 0 3 3 0 0 1 6 0Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 10.5c0 7.142-7.5 11.25-7.5 11.25S4.5 17.642 4.5 10.5a7.5 7.5 0 1 1 15 0Z"></path></svg></button></div><a href="https://scholar.google.com/citations?user=btThEsYAAAAJ" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://orcid.org/my-orcid?orcid=0000-0003-1782-8704" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="ORCID"><svg viewBox="0 0 24 24" fill="currentColor" class="h-5 w-5" xmlns="http://www.w3.org/2000/svg"><path d="M12 0C5.372 0 0 5.372 0 12s5.372 12 12 12 12-5.372 12-12S18.628 0 12 0zM7.369 4.378c.525 0 .947.431.947.947s-.422.947-.947.947a.95.95 0 0 1-.947-.947c0-.525.422-.947.947-.947zm-.722 3.038h1.444v10.041H6.647V7.416zm3.562 0h3.9c3.712 0 5.344 2.653 5.344 5.025 0 2.578-2.016 5.025-5.325 5.025h-3.919V7.416zm1.444 1.303v7.444h2.297c3.272 0 4.022-2.484 4.022-3.722 0-2.016-1.284-3.722-4.097-3.722h-2.222z"></path></svg></a><a href="https://github.com/WangJingyao07" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="https://www.zhihu.com/people/wang-dou-ya-11" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Zhihu"><svg viewBox="0 0 24 24" fill="currentColor" class="h-5 w-5" xmlns="http://www.w3.org/2000/svg"><path d="M5.721 0C2.251 0 0 2.25 0 5.719V18.28C0 21.751 2.252 24 5.721 24h12.56C21.751 24 24 21.75 24 18.281V5.72C24 2.249 21.75 0 18.281 0zm1.964 4.078c-.271.73-.5 1.434-.68 2.11h4.587c.545-.006.445 1.168.445 1.171H9.384a58.104 58.104 0 01-.112 3.797h2.712c.388.023.393 1.251.393 1.266H9.183a9.223 9.223 0 01-.408 2.102l.757-.604c.452.456 1.512 1.712 1.906 2.177.473.681.063 2.081.063 2.081l-2.794-3.382c-.653 2.518-1.845 3.607-1.845 3.607-.523.468-1.58.82-2.64.516 2.218-1.73 3.44-3.917 3.667-6.497H4.491c0-.015.197-1.243.806-1.266h2.71c.024-.32.086-3.254.086-3.797H6.598c-.136.406-.158.447-.268.753-.594 1.095-1.603 1.122-1.907 1.155.906-1.821 1.416-3.6 1.591-4.064.425-1.124 1.671-1.125 1.671-1.125zM13.078 6h6.377v11.33h-2.573l-2.184 1.373-.401-1.373h-1.219zm1.313 1.219v8.86h.623l.263.937 1.455-.938h1.456v-8.86z"></path></svg></a></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="space-y-2 text-sm text-neutral-700 dark:text-neutral-500"><div>MLLM/LLM Reasoning</div><div>Embodied AI</div><div>Transfer Learning</div></div></div><div class="flex justify-center"><div class="relative"><button class="flex items-center space-x-2 px-4 py-2 rounded-lg font-medium text-sm transition-all duration-200 bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-500 hover:bg-red-50 dark:hover:bg-red-900/20 hover:text-red-600 dark:hover:text-red-400 cursor-pointer" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg><span>Like</span></button></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">I am a second-year PhD student at <a href="http://www.is.cas.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">ISCAS</a>, <a href="https://www.ucas.ac.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">UCAS</a>, advised by <a href="https://people.ucas.ac.cn/~cwzheng?language=en" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Changwen Zheng</a></p>
<p class="mb-4 last:mb-0">Prior to this, I obtained a B.E. degree with First Class Honours in Beijing University of Technology.</p>
<p class="mb-4 last:mb-0">My research interest includes MLLM/LLM Reasoning, Embodied AI, and Transfer Learning. I am maintaining two popular projects on LLM post-training and meta-learning <a href="https://github.com/WangJingyao07/Awesome-GRPO" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Awesome-GRPO</a> and <a href="https://wangjingyao07.github.io/Awesome-Meta-Learning-Platform/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Awesome-META+</a></p></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">News</h2><div class="space-y-3"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2026-02</span><p class="text-sm text-neutral-700">One work has been accepted by CVPR 2026 ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2026-01</span><p class="text-sm text-neutral-700">One work has been accepted by WWW 2026 ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-11</span><p class="text-sm text-neutral-700">One work has been accepted by ASOC ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-10</span><p class="text-sm text-neutral-700">Two works have been accepted by AAAI 2026 ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-09</span><p class="text-sm text-neutral-700">One work has been accepted by NeurIPS 2025 ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-07</span><p class="text-sm text-neutral-700">One work has been accepted by ACMMM 2025 ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-07</span><p class="text-sm text-neutral-700">Two works have been accepted by ICML 2025 ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2024-07</span><p class="text-sm text-neutral-700">One work has been accepted by TMM ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2024-05</span><p class="text-sm text-neutral-700">One work has been accepted by IJCV ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2024-04</span><p class="text-sm text-neutral-700">One work has been accepted by IJCAI 2024 ðŸŽ‰</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2023-02</span><p class="text-sm text-neutral-700">One work has been accepted by ACM TOMM ðŸŽ‰</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/publications/">View All<!-- --> â†’</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Towards task sampler learning for meta-learning</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Jingyao Wang</span>, </span><span><span class=" ">Wenwen Qiang</span>, </span><span><span class=" ">Xingzhe Su</span>, </span><span><span class=" ">Changwen Zheng</span>, </span><span><span class=" ">Fuchun Sun</span>, </span><span><span class=" ">Hui Xiong</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">International Journal of Computer Vision</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Image-based freeform handwriting authentication with energy-oriented self-supervised learning</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Jingyao Wang</span>, </span><span><span class=" ">Luntian Mou</span>, </span><span><span class=" ">Changwen Zheng</span>, </span><span><span class=" ">Wen Gao</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">IEEE Transactions on Multimedia</p></div></div></section></section></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated<!-- -->: <!-- -->March 1, 2026</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">ðŸš€</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n3:I[3719,[\"461\",\"static/chunks/461-a9a5d7b05a71d978.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"291\",\"static/chunks/291-e39730cc9327e39b.js\",\"177\",\"static/chunks/app/layout-4cf029633e6d5428.js\"],\"ThemeProvider\"]\n4:I[310,[\"461\",\"static/chunks/461-a9a5d7b05a71d978.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"291\",\"static/chunks/291-e39730cc9327e39b.js\",\"177\",\"static/chunks/app/layout-4cf029633e6d5428.js\"],\"LocaleProvider\"]\n5:I[4574,[\"461\",\"static/chunks/461-a9a5d7b05a71d978.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"291\",\"static/chunks/291-e39730cc9327e39b.js\",\"177\",\"static/chunks/app/layout-4cf029633e6d5428.js\"],\"default\"]\n6:I[7555,[],\"\"]\n7:I[1295,[],\"\"]\n8:I[2548,[\"461\",\"static/chunks/461-a9a5d7b05a71d978.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"291\",\"static/chunks/291-e39730cc9327e39b.js\",\"177\",\"static/chunks/app/layout-4cf029633e6d5428.js\"],\"default\"]\n9:I[6613,[\"461\",\"static/chunks/461-a9a5d7b05a71d978.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-97f90665cfe632a2.js\",\"974\",\"static/chunks/app/page-6c80dddde2d5f2d2.js\"],\"default\"]\ne:I[9665,[],\"MetadataBoundary\"]\n10:I[9665,[],\"OutletBoundary\"]\n13:I[4911,[],\"AsyncMetadataOutlet\"]\n15:I[9665,[],\"ViewportBoundary\"]\n17:I[6614,[],\"\"]\n:HL[\"/_next/static/css/78c4370b96c05382.css\",\"style\"]\n2:T57d,\n    try {\n      const cfg = {\"enabled\":true,\"locales\":[\"en\"],\"defaultLocale\":\"en\",\"mode\":\"auto\",\"fixedLocale\":\"en\",\"persist\":true,\"switcher\":true,\"labels\":{\"en\":\"English\"}};\n      const storageKey = 'locale-storage';\n      const normalize = (value) =\u003e typeof value === 'string' ? value.trim().replace('_', '-').toLowerCase() : '';\n      const matchLocale = (candidate) =\u003e {\n        const normalized = normalize(candidate);\n        if (!normalized) return null;\n        if (cfg.locales.includes(normalized)) return normalized;\n        const language = normalized.split('-')[0];\n        if (cfg.locales.includes(language)) return language;\n        return null;\n    "])</script><script>self.__next_f.push([1,"  };\n\n      let resolved = null;\n\n      if (cfg.persist) {\n        resolved = matchLocale(localStorage.getItem(storageKey));\n      }\n\n      if (!resolved) {\n        if (cfg.mode === 'fixed') {\n          resolved = cfg.fixedLocale;\n        } else {\n          resolved = matchLocale(navigator.language);\n        }\n      }\n\n      if (!resolved) {\n        resolved = cfg.defaultLocale;\n      }\n\n      const root = document.documentElement;\n      root.lang = resolved;\n      root.setAttribute('data-locale', resolved);\n\n      if (cfg.persist) {\n        localStorage.setItem(storageKey, resolved);\n      }\n    } catch (e) {\n      const root = document.documentElement;\n      root.lang = 'en';\n      root.setAttribute('data-locale', 'en');\n    }\n  a:T4cf,Meta-learning aims to learn general knowledge with diverse training tasks conducted from limited data, and then transfer it to new tasks. It is commonly believed that increasing task diversity will enhance the generalization ability of meta-learning models. However, this paper challenges this view through empirical and theoretical analysis. We obtain three conclusions: (i) there is no universal task sampling strategy that can guarantee the optimal performance of meta-learning models; (ii) over-constraining task diversity may incur the risk of under-fitting or over-fitting during training; and (iii) the generalization performance of meta-learning models are affected by task diversity, task entropy, and task difficulty. Based on this insight, we design a novel task sampler, called Adaptive Sampler (ASr). ASr is a plug-and-play module that can be integrated into any meta-learning framework. It dynamically adjusts task weights according to task diversity, task entropy, and task difficulty, thereby obtaining the optimal probability distribution for meta-training tasks. Finally, we conduct experiments on a series of benchmark datasets across various scenarios, and the results demonstrate that ASr has clear advantages.b:T694,@article{wang2024towards,\n  title = {Towards task sampler lear"])</script><script>self.__next_f.push([1,"ning for meta-learning},\n  author = {Jingyao Wang and Wenwen Qiang and Xingzhe Su and Changwen Zheng and Fuchun Sun and Hui Xiong},\n  journal = {International Journal of Computer Vision},\n  volume = {132},\n  number = {12},\n  pages = {5534--5564},\n  year = {2024},\n  doi = {10.1007/s11263-024-02145-0},\n  pdf = {https://link.springer.com/article/10.1007/s11263-024-02145-0},\n  abstract = {Meta-learning aims to learn general knowledge with diverse training tasks conducted from limited data, and then transfer it to new tasks. It is commonly believed that increasing task diversity will enhance the generalization ability of meta-learning models. However, this paper challenges this view through empirical and theoretical analysis. We obtain three conclusions: (i) there is no universal task sampling strategy that can guarantee the optimal performance of meta-learning models; (ii) over-constraining task diversity may incur the risk of under-fitting or over-fitting during training; and (iii) the generalization performance of meta-learning models are affected by task diversity, task entropy, and task difficulty. Based on this insight, we design a novel task sampler, called Adaptive Sampler (ASr). ASr is a plug-and-play module that can be integrated into any meta-learning framework. It dynamically adjusts task weights according to task diversity, task entropy, and task difficulty, thereby obtaining the optimal probability distribution for meta-training tasks. Finally, we conduct experiments on a series of benchmark datasets across various scenarios, and the results demonstrate that ASr has clear advantages.}\n}c:T655,Freeform handwriting authentication verifies a person's identity from their writing style and habits in messy handwriting data. This technique has gained widespread attention in recent years as a valuable tool for various fields, e.g., fraud prevention and cultural heritage protection. However, it still remains a challenging task in reality due to three reasons: (i) severe damage, (ii) complex high-dimensional featu"])</script><script>self.__next_f.push([1,"res, and (iii) lack of supervision. To address these issues, we propose SherlockNet, an energy-oriented two-branch contrastive self-supervised learning framework for robust and fast freeform handwriting authentication. It consists of four stages: (i) pre-processing: converting manuscripts into energy distributions using a novel plug-and-play energy-oriented operator to eliminate the influence of noise; (ii) generalized pre-training: learning general representation through two-branch momentum-based adaptive contrastive learning with the energy distributions, which handles the high-dimensional features and spatial dependencies of handwriting; (iii) personalized fine-tuning: calibrating the learned knowledge using a small amount of labeled data from downstream tasks; and (iv) practical application: identifying individual handwriting from scrambled, missing, or forged data efficiently and conveniently. Considering the practicality, we construct EN-HA, a novel dataset that simulates data forgery and severe damage in real applications. Finally, we conduct extensive experiments on six benchmark datasets including our EN-HA, and the results prove the robustness and efficiency of SherlockNet.d:T7fb,@article{wang2024image,\n  title = {Image-based freeform handwriting authentication with energy-oriented self-supervised learning},\n  author = {Jingyao Wang and Luntian Mou and Changwen Zheng and Wen Gao},\n  journal = {IEEE Transactions on Multimedia},\n  volume = {27},\n  pages = {1397--1409},\n  year = {2024},\n  doi = {10.1109/TMM.2024.3521807},\n  pdf = {https://dl.acm.org/doi/10.1109/TMM.2024.3521807},\n  abstract = {Freeform handwriting authentication verifies a person's identity from their writing style and habits in messy handwriting data. This technique has gained widespread attention in recent years as a valuable tool for various fields, e.g., fraud prevention and cultural heritage protection. However, it still remains a challenging task in reality due to three reasons: (i) severe damage, (ii) complex high-dimensional featur"])</script><script>self.__next_f.push([1,"es, and (iii) lack of supervision. To address these issues, we propose SherlockNet, an energy-oriented two-branch contrastive self-supervised learning framework for robust and fast freeform handwriting authentication. It consists of four stages: (i) pre-processing: converting manuscripts into energy distributions using a novel plug-and-play energy-oriented operator to eliminate the influence of noise; (ii) generalized pre-training: learning general representation through two-branch momentum-based adaptive contrastive learning with the energy distributions, which handles the high-dimensional features and spatial dependencies of handwriting; (iii) personalized fine-tuning: calibrating the learned knowledge using a small amount of labeled data from downstream tasks; and (iv) practical application: identifying individual handwriting from scrambled, missing, or forged data efficiently and conveniently. Considering the practicality, we construct EN-HA, a novel dataset that simulates data forgery and severe damage in real applications. Finally, we conduct extensive experiments on six benchmark datasets including our EN-HA, and the results prove the robustness and efficiency of SherlockNet.}\n}"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"V94FKsjHIt6eSSFRu6WMf\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/78c4370b96c05382.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"font\",\"type\":\"font/woff2\",\"href\":\"https://jialeliu.com/fonts/georgiab.woff2\",\"crossOrigin\":\"\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$2\"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L3\",null,{\"children\":[\"$\",\"$L4\",null,{\"config\":{\"enabled\":true,\"locales\":[\"en\"],\"defaultLocale\":\"en\",\"mode\":\"auto\",\"fixedLocale\":\"en\",\"persist\":true,\"switcher\":true,\"labels\":{\"en\":\"English\"}},\"children\":[[\"$\",\"$L5\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Teaching\",\"type\":\"page\",\"target\":\"teaching\",\"href\":\"/teaching\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}],\"siteTitle\":\"Jingyao Wang\",\"enableOnePageMode\":false,\"i18n\":\"$0:f:0:1:1:props:children:1:props:children:1:props:children:props:children:props:config\",\"itemsByLocale\":{\"en\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Teaching\",\"type\":\"page\",\"target\":\"teaching\",\"href\":\"/teaching\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}]},\"siteTitleByLocale\":{\"en\":\"Jingyao Wang\"}}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L7\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L8\",null,{\"lastUpdated\":\"March 1, 2026\",\"lastUpdatedByLocale\":{\"en\":\"March 1, 2026\"},\"defaultLocale\":\"en\"}]]}]}]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"$L9\",null,{\"dataByLocale\":{\"en\":{\"author\":{\"name\":\"Jingyao Wang\",\"title\":\"PhD Student\",\"institution\":\"Institute of Software Chinese Academy of Sciences\",\"avatar\":\"/bio.jpg\"},\"social\":{\"email\":\"wangjingyao2023@iscas.ac.cn\",\"location\":\"Beijing, China\",\"location_url\":\"https://maps.google.com\",\"location_details\":[\"No.4 South 4th Street, Zhongguancun, Haidian District,\",\"Beijing, 100190, P.R. China\"],\"google_scholar\":\"https://scholar.google.com/citations?user=btThEsYAAAAJ\",\"orcid\":\"https://orcid.org/my-orcid?orcid=0000-0003-1782-8704\",\"github\":\"https://github.com/WangJingyao07\",\"zhihu\":\"https://www.zhihu.com/people/wang-dou-ya-11\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":false},\"enableOnePageMode\":false,\"researchInterests\":[\"MLLM/LLM Reasoning\",\"Embodied AI\",\"Transfer Learning\"],\"pagesToShow\":[{\"type\":\"about\",\"id\":\"about\",\"sections\":[{\"id\":\"about\",\"type\":\"markdown\",\"source\":\"bio.md\",\"title\":\"About\",\"content\":\"I am a second-year PhD student at [ISCAS](http://www.is.cas.cn/), [UCAS](https://www.ucas.ac.cn/), advised by [Prof. Changwen Zheng](https://people.ucas.ac.cn/~cwzheng?language=en)\\n\\nPrior to this, I obtained a B.E. degree with First Class Honours in Beijing University of Technology.\\n\\nMy research interest includes MLLM/LLM Reasoning, Embodied AI, and Transfer Learning. I am maintaining two popular projects on LLM post-training and meta-learning [Awesome-GRPO](https://github.com/WangJingyao07/Awesome-GRPO) and [Awesome-META+](https://wangjingyao07.github.io/Awesome-Meta-Learning-Platform/)\"},{\"id\":\"news\",\"type\":\"list\",\"title\":\"News\",\"source\":\"news.toml\",\"items\":[{\"date\":\"2026-02\",\"content\":\"One work has been accepted by CVPR 2026 ðŸŽ‰\"},{\"date\":\"2026-01\",\"content\":\"One work has been accepted by WWW 2026 ðŸŽ‰\"},{\"date\":\"2025-11\",\"content\":\"One work has been accepted by ASOC ðŸŽ‰\"},{\"date\":\"2025-10\",\"content\":\"Two works have been accepted by AAAI 2026 ðŸŽ‰\"},{\"date\":\"2025-09\",\"content\":\"One work has been accepted by NeurIPS 2025 ðŸŽ‰\"},{\"date\":\"2025-07\",\"content\":\"One work has been accepted by ACMMM 2025 ðŸŽ‰\"},{\"date\":\"2025-07\",\"content\":\"Two works have been accepted by ICML 2025 ðŸŽ‰\"},{\"date\":\"2024-07\",\"content\":\"One work has been accepted by TMM ðŸŽ‰\"},{\"date\":\"2024-05\",\"content\":\"One work has been accepted by IJCV ðŸŽ‰\"},{\"date\":\"2024-04\",\"content\":\"One work has been accepted by IJCAI 2024 ðŸŽ‰\"},{\"date\":\"2023-02\",\"content\":\"One work has been accepted by ACM TOMM ðŸŽ‰\"}]},{\"id\":\"featured_publications\",\"type\":\"publications\",\"title\":\"Selected Publications\",\"filter\":\"selected\",\"limit\":5,\"publications\":[{\"id\":\"wang2024towards\",\"title\":\"Towards task sampler learning for meta-learning\",\"authors\":[{\"name\":\"Jingyao Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wenwen Qiang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xingzhe Su\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Changwen Zheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Fuchun Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hui Xiong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:dataByLocale:en:pagesToShow:0:sections:2:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"International Journal of Computer Vision\",\"conference\":\"\",\"volume\":\"132\",\"issue\":\"12\",\"pages\":\"5534--5564\",\"doi\":\"10.1007/s11263-024-02145-0\",\"code\":\"https://github.com/WangJingyao07/Adaptive-Sampler\",\"abstract\":\"$a\",\"description\":\"\",\"selected\":true,\"preview\":\"ijcv.png\",\"bibtex\":\"$b\"},{\"id\":\"wang2024image\",\"title\":\"Image-based freeform handwriting authentication with energy-oriented self-supervised learning\",\"authors\":[{\"name\":\"Jingyao Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Luntian Mou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Changwen Zheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Wen Gao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:dataByLocale:en:pagesToShow:0:sections:2:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"IEEE Transactions on Multimedia\",\"conference\":\"\",\"volume\":\"27\",\"pages\":\"1397--1409\",\"doi\":\"10.1109/TMM.2024.3521807\",\"abstract\":\"$c\",\"description\":\"\",\"selected\":true,\"preview\":\"tmm.png\",\"bibtex\":\"$d\"}]}]}]}},\"defaultLocale\":\"en\"}],[\"$\",\"$Le\",null,{\"children\":\"$Lf\"}],null,[\"$\",\"$L10\",null,{\"children\":[\"$L11\",\"$L12\",[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"805FErJZB0nq8GSROhxyI\",{\"children\":[[\"$\",\"$L15\",null,{\"children\":\"$L16\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$17\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"18:\"$Sreact.suspense\"\n19:I[4911,[],\"AsyncMetadata\"]\nf:[\"$\",\"$18\",null,{\"fallback\":null,\"children\":[\"$\",\"$L19\",null,{\"promise\":\"$@1a\"}]}]\n"])</script><script>self.__next_f.push([1,"12:null\n"])</script><script>self.__next_f.push([1,"16:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n11:null\n"])</script><script>self.__next_f.push([1,"1a:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Jingyao Wang\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"PhD student at the University of Example.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Jingyao Wang\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Jingyao Wang,PhD,Research,Institute of Software Chinese Academy of Sciences\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Jingyao Wang\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Jingyao Wang\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Jingyao Wang\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"PhD student at the University of Example.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Jingyao Wang's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Jingyao Wang\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"PhD student at the University of Example.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\"}]],\"error\":null,\"digest\":\"$undefined\"}\n14:{\"metadata\":\"$1a:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>